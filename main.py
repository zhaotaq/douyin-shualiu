#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŠ–éŸ³åˆ·é‡æ‰¹é‡æäº¤ç³»ç»Ÿ
ç®€æ´ç‰ˆä¸»ç¨‹åº
"""

from douyin_batch_submitter_v2 import DouyinBatchSubmitterV2
import json
import time
from datetime import datetime
import subprocess
import requests
import os
import base64
import yaml
import random

def load_urls_from_file(filename: str = "douyin_urls.txt") -> list:
    """ä»æ–‡ä»¶åŠ è½½URLåˆ—è¡¨"""
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip()]
        return urls
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶ {filename} ä¸å­˜åœ¨")
        return []

def save_urls_to_file(urls: list, filename: str = "douyin_urls.txt"):
    """ä¿å­˜URLåˆ—è¡¨åˆ°æ–‡ä»¶"""
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            for url in urls:
                f.write(url + '\n')
        print(f"ğŸ“ é“¾æ¥å·²ä¿å­˜åˆ° {filename}")
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")

def quick_test():
    """å¿«é€Ÿæµ‹è¯•åŠŸèƒ½"""
    print("ğŸš€ å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
    print("=" * 40)
    
    submitter = DouyinBatchSubmitterV2()
    
    # æµ‹è¯•è¿æ¥
    if submitter.test_connection():
        print("âœ… APIè¿æ¥æ­£å¸¸")
    else:
        print("âŒ APIè¿æ¥å¼‚å¸¸")
        return False
    
    # æµ‹è¯•é“¾æ¥
    test_url = input("è¯·è¾“å…¥æµ‹è¯•é“¾æ¥ï¼ˆå›è½¦ä½¿ç”¨é»˜è®¤ï¼‰: ").strip()
    if not test_url:
        test_url = f"https://www.douyin.com/video/75023406983692{datetime.now().strftime('%H%M%S')}"
    
    print(f"ğŸ“¤ æµ‹è¯•é“¾æ¥: {test_url}")
    
    success, message, order_info = submitter.submit_single_url(test_url)
    
    if success:
        print("âœ… æäº¤æˆåŠŸï¼")
        print("ğŸ“‹ è®¢å•ä¿¡æ¯:")
        for key, value in order_info.items():
            if key != 'response':
                print(f"   {key}: {value}")
        return True
    else:
        print(f"ğŸ“‹ æäº¤ç»“æœ: {message}")
        
        if "é‡å¤æäº¤" in message or "å·²é¢†å–è¿‡" in message:
            print("â„¹ï¸ è¿™è¯´æ˜APIå·¥ä½œæ­£å¸¸ï¼Œåªæ˜¯æœ‰é‡å¤/é™åˆ¶")
            return True
        else:
            print("âŒ å¯èƒ½å­˜åœ¨å…¶ä»–é—®é¢˜")
            return False

def batch_submit():
    """æ‰¹é‡æäº¤åŠŸèƒ½"""
    print("ğŸš€ æ‰¹é‡æäº¤æ¨¡å¼")
    print("=" * 40)
    
    # é€‰æ‹©è¾“å…¥æ–¹å¼
    print("è¯·é€‰æ‹©è¾“å…¥æ–¹å¼:")
    print("1. ä»æ–‡ä»¶è¯»å– (douyin_urls.txt)")
    print("2. æ‰‹åŠ¨è¾“å…¥")
    
    choice = input("é€‰æ‹© (1-2): ").strip()
    
    urls = []
    if choice == '1':
        urls = load_urls_from_file()
        if not urls:
            print("âŒ æ–‡ä»¶ä¸ºç©ºæˆ–ä¸å­˜åœ¨")
            return
    elif choice == '2':
        print("è¯·è¾“å…¥æŠ–éŸ³è§†é¢‘é“¾æ¥ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸï¼‰:")
        while True:
            url = input("é“¾æ¥: ").strip()
            if not url:
                break
            urls.append(url)
        
        if urls:
            save_choice = input("æ˜¯å¦ä¿å­˜é“¾æ¥åˆ°æ–‡ä»¶ï¼Ÿ(y/n): ").lower()
            if save_choice == 'y':
                save_urls_to_file(urls)
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")
        return
    
    if not urls:
        print("âŒ æ²¡æœ‰å¯æäº¤çš„é“¾æ¥")
        return
    
    print(f"ğŸ“‹ å…±æœ‰ {len(urls)} ä¸ªé“¾æ¥å¾…æäº¤")
    
    # é…ç½®å‚æ•°
    delay_min = float(input("æœ€å°å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤3ï¼‰: ") or "3")
    delay_max = float(input("æœ€å¤§å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤8ï¼‰: ") or "8")
    
    confirm = input(f"\nç¡®è®¤æäº¤ {len(urls)} ä¸ªé“¾æ¥ï¼Ÿ(y/n): ").lower()
    if confirm != 'y':
        print("âŒ å·²å–æ¶ˆ")
        return
    
    print(f"\nğŸš€ å¼€å§‹æ‰¹é‡æäº¤...")
    
    submitter = DouyinBatchSubmitterV2(group_name='åˆ·æµä¸“ç”¨')
    results = submitter.batch_submit(
        urls=urls,
        max_workers=1,
        delay_range=(delay_min, delay_max)
    )
    
    # åˆ†æç»“æœ
    success_count = sum(1 for r in results if r['success'])
    total_count = len(results)
    
    print(f"\nğŸ“Š æäº¤å®Œæˆ:")
    print(f"âœ… æˆåŠŸ: {success_count}/{total_count}")
    if total_count > 0:
        print(f"ğŸ“ˆ æˆåŠŸç‡: {(success_count/total_count*100):.1f}%")
    else:
        print("ğŸ“ˆ æˆåŠŸç‡: 0.0%ï¼ˆæ— æœ‰æ•ˆæäº¤ï¼‰")
    
    submitter.save_results(results)
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶")

def download_and_generate_config(sub_url, config_path, proxy_port, api_port):
    resp = requests.get(sub_url)
    try:
        content = base64.b64decode(resp.text).decode()
    except Exception:
        content = resp.text  # å¯èƒ½ç›´æ¥æ˜¯yaml
    config = yaml.safe_load(content)
    config['port'] = proxy_port
    config['external-controller'] = f'127.0.0.1:{api_port}'
    # ä¿ç•™æ‰€æœ‰åˆ†ç»„ï¼Œè‡ªåŠ¨è¯†åˆ«Selectoråˆ†ç»„
    with open(config_path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, allow_unicode=True)

def start_mihomo(mihomo_path, config_path):
    proc = subprocess.Popen([
        mihomo_path,
        '-d', '.',
        '-f', config_path
    ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    return proc

def wait_mihomo_api(api_port, timeout=30):
    for _ in range(timeout):
        try:
            r = requests.get(f'http://127.0.0.1:{api_port}/proxies', timeout=1)
            if r.status_code == 200:
                return True
        except Exception:
            time.sleep(1)
    return False

def get_main_group_and_all_nodes(api_port):
    r = requests.get(f'http://127.0.0.1:{api_port}/proxies')
    data = r.json()
    # å…¼å®¹é¡¶å±‚ä¸º{"proxies": {...}}çš„ç»“æ„
    if 'proxies' in data:
        data = data['proxies']
    # è‡ªåŠ¨è¯†åˆ«ç¬¬ä¸€ä¸ªæœ‰allå­—æ®µä¸”ä¸æ˜¯providerçš„åˆ†ç»„ä¸ºä¸»åˆ†ç»„
    group_names = [k for k, v in data.items() if v.get('all') and not v.get('type', '').lower().endswith('provider')]
    if not group_names:
        raise RuntimeError('æœªæ‰¾åˆ°ä»»ä½•åˆ†ç»„')
    main_group = group_names[0]
    # é€’å½’å±•å¼€æ‰€æœ‰çœŸå®èŠ‚ç‚¹
    def collect_nodes(group_name, visited=None):
        if visited is None:
            visited = set()
        if group_name in visited:
            return []
        visited.add(group_name)
        group = data.get(group_name, {})
        nodes = group.get('all') or []
        real_nodes = []
        for n in nodes:
            n_type = data.get(n, {}).get('type', '').lower()
            if n in data and n_type in ('selector', 'urltest', 'fallback'):
                real_nodes.extend(collect_nodes(n, visited))
            else:
                real_nodes.append(n)
        return real_nodes
    all_nodes = collect_nodes(main_group)
    if not all_nodes:
        raise RuntimeError('ä¸»åˆ†ç»„ä¸‹æœªæ‰¾åˆ°ä»»ä½•çœŸå®èŠ‚ç‚¹')
    return main_group, all_nodes

def switch_random_node_main_group(api_port, group_name, all_nodes):
    node = random.choice(all_nodes)
    resp = requests.put(f'http://127.0.0.1:{api_port}/proxies/{group_name}', json={'name': node})
    resp.raise_for_status()
    now = requests.get(f'http://127.0.0.1:{api_port}/proxies/{group_name}').json().get('now')
    if now != node:
        raise RuntimeError(f'åˆ‡æ¢èŠ‚ç‚¹å¤±è´¥: æœŸæœ›{node}, å®é™…{now}')
    return node

def is_node_available(proxy_port):
    proxies = {
        'http': f'http://127.0.0.1:{proxy_port}',
        'https': f'http://127.0.0.1:{proxy_port}',
    }
    try:
        resp = requests.get('https://longsiye.nyyo.cn', proxies=proxies, timeout=5, verify=False)
        return resp.status_code == 200
    except Exception:
        return False

def main():
    print('ğŸ¯ æŠ–éŸ³åˆ·é‡æ‰¹é‡æäº¤ç³»ç»Ÿï¼ˆè‡ªåŠ¨ä»£ç†é›†æˆç‰ˆï¼‰')
    print('=' * 40)
    config_path = './clash_config.yaml'
    mihomo_path = './mihomo-windows-amd64.exe'
    proxy_port = 9950
    api_port = 9900
    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(config_path):
        sub_url = input('è¯·è¾“å…¥ä»£ç†è®¢é˜…é“¾æ¥: ').strip()
        print('â¬ æ­£åœ¨ä¸‹è½½è®¢é˜…å¹¶ç”Ÿæˆé…ç½®...')
        download_and_generate_config(sub_url, config_path, proxy_port, api_port)
    else:
        # æ£€æŸ¥å¹¶ä¿®æ­£ç«¯å£
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        changed = False
        if config.get('mixed-port') != proxy_port:
            config['mixed-port'] = proxy_port
            changed = True
        if config.get('external-controller') != f'127.0.0.1:{api_port}':
            config['external-controller'] = f'127.0.0.1:{api_port}'
            changed = True
        if changed:
            with open(config_path, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, allow_unicode=True)
            print(f'å·²è‡ªåŠ¨ä¿®æ­£é…ç½®æ–‡ä»¶ç«¯å£ä¸º {proxy_port}/{api_port}')
        else:
            print(f'æ£€æµ‹åˆ°å·²å­˜åœ¨é…ç½®æ–‡ä»¶ {config_path}ï¼Œå°†ç›´æ¥å¤ç”¨ã€‚')
    # è·å–æ‰€æœ‰èŠ‚ç‚¹ä¿¡æ¯ï¼ˆåªéœ€ä¸€æ¬¡ï¼‰
    print('ğŸš€ å¯åŠ¨mihomoä»£ç†å†…æ ¸...')
    mihomo_proc = start_mihomo(mihomo_path, config_path)
    print('â³ ç­‰å¾…ä»£ç†APIç«¯å£å¯åŠ¨...')
    if not wait_mihomo_api(api_port):
        print('âŒ mihomoå¯åŠ¨å¤±è´¥')
        mihomo_proc.terminate()
        return
    print('âœ… mihomoå·²å¯åŠ¨ï¼ŒAPIå¯ç”¨')
    group_name, all_nodes = get_main_group_and_all_nodes(api_port)
    print(f'æ£€æµ‹åˆ°ä¸»åˆ†ç»„: {group_name}ï¼Œå…±{len(all_nodes)}ä¸ªçœŸå®èŠ‚ç‚¹')
    mihomo_proc.terminate()
    time.sleep(2)
    # é€‰æ‹©åˆ·æµæ¨¡å¼
    urls = []
    print('è¯·è¾“å…¥æŠ–éŸ³è§†é¢‘é“¾æ¥ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸï¼‰:')
    while True:
        url = input('é“¾æ¥: ').strip()
        if not url:
            break
        urls.append(url)
    if not urls:
        print('âŒ æ²¡æœ‰å¯æäº¤çš„é“¾æ¥')
        return
    print(f'ğŸ“‹ å…±æœ‰ {len(urls)} ä¸ªé“¾æ¥å¾…æäº¤')
    delay_min = float(input('æœ€å°å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤3ï¼‰: ') or '3')
    delay_max = float(input('æœ€å¤§å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤8ï¼‰: ') or '8')
    confirm = input(f'\nç¡®è®¤æäº¤ {len(urls)} ä¸ªé“¾æ¥ï¼Ÿ(y/n): ').lower()
    if confirm != 'y':
        print('âŒ å·²å–æ¶ˆ')
        return
    
    print(f'\nğŸš€ å¼€å§‹æ‰¹é‡æäº¤...')
    
    # Estimate total time based on delay
    avg_delay = (delay_min + delay_max) / 2
    estimated_min_time = len(urls) * avg_delay
    print(f'â³ ä¼°ç®—æœ€å°å®Œæˆæ—¶é—´ (ä»…è€ƒè™‘å»¶è¿Ÿ): çº¦ {estimated_min_time:.1f} ç§’')

    results = []
    for i, url in enumerate(urls, 1):
        # Print progress
        print(f'\n--- å¤„ç†è¿›åº¦: {i}/{len(urls)} ---')

        tried_nodes = set()
        success = False
        message = ''
        order_info = {}
        node_used_for_success = None
        
        # Attempt to submit with different nodes, up to 10 times or until successful/other error
        max_attempts = 10
        attempt_count = 0
        
        available_nodes_for_url = list(all_nodes) # Create a mutable list to shuffle and pick from
        random.shuffle(available_nodes_for_url)

        # Use a while loop to control attempts based on result
        while attempt_count < max_attempts and available_nodes_for_url:
            attempt_count += 1
            
            # Select a node that hasn't been tried for this URL in this attempt block yet
            # We shuffle the list once before the loop, so just pick the next available one
            node = available_nodes_for_url.pop(0)
            tried_nodes.add(node) # Keep track of nodes tried in total for this URL
            
            print(f'ğŸ”„ å°è¯•èŠ‚ç‚¹ ({attempt_count}/{max_attempts}): {node}')

            # Start mihomo for this attempt
            mihomo_proc = start_mihomo(mihomo_path, config_path)
            if not wait_mihomo_api(api_port):
                print('âŒ mihomoå¯åŠ¨å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªèŠ‚ç‚¹')
                mihomo_proc.terminate()
                time.sleep(2) # Short delay before next attempt
                continue # Continue to next node attempt
            
            try:
                # Switch node
                try:
                    switch_random_node_main_group(api_port, group_name, [node])
                except Exception as e:
                    print(f'âŒ èŠ‚ç‚¹åˆ‡æ¢å¤±è´¥ ({node}): {e}ï¼Œå°è¯•ä¸‹ä¸€ä¸ªèŠ‚ç‚¹')
                    mihomo_proc.terminate()
                    time.sleep(2) # Short delay before next attempt
                    continue # Continue to next node attempt

                # å¯ç”¨èŠ‚ç‚¹ï¼Œæäº¤åˆ·æµ
                submitter = DouyinBatchSubmitterV2(base_url="https://longsiye.nyyo.cn")
                
                submit_success = False
                submit_message = ''
                submit_order_info = {}

                try:
                    submit_success, submit_message, submit_order_info = submitter.submit_single_url(url)
                except requests.exceptions.SSLError as ssl_error:
                    print(f'âŒ æäº¤æ—¶å‘ç”ŸSSLè¿æ¥é”™è¯¯ ({node}): {ssl_error}ï¼Œç«‹å³å°è¯•ä¸‹ä¸€ä¸ªèŠ‚ç‚¹')
                    # Terminate mihomo and continue to the next node attempt
                    mihomo_proc.terminate()
                    time.sleep(2) # Short delay after exception
                    # Set values to reflect failure for this node attempt
                    success = False
                    message = f'SSLè¿æ¥é”™è¯¯ ({node}): {ssl_error}'
                    order_info = {}
                    continue # Continue the while loop to try the next node
                except Exception as api_error:
                     # Catch other potential exceptions from submit_single_url
                     print(f'âŒ æäº¤æ—¶å‘ç”Ÿå…¶ä»–å¼‚å¸¸ ({node}): {api_error}ï¼Œåœæ­¢å°è¯•è¯¥é“¾æ¥çš„å…¶ä»–èŠ‚ç‚¹')
                     # Terminate mihomo
                     mihomo_proc.terminate()
                     time.sleep(2) # Short delay after exception
                     # Set values to reflect failure and break
                     success = False
                     message = f'æäº¤å¼‚å¸¸ ({node}): {api_error}'
                     order_info = {}
                     break # Break the node attempt loop for this URL

                mihomo_proc.terminate()
                time.sleep(2) # Delay after submission attempt
                
                success = submit_success
                message = submit_message
                order_info = submit_order_info

                # Check if successful or if it's the specific 'already received' message
                if success:
                    node_used_for_success = node
                    print(f'âœ… æäº¤æˆåŠŸï¼ä½¿ç”¨èŠ‚ç‚¹: {node}')
                    break # Success, break the node attempt loop
                elif "æ‚¨ä»Šå¤©å·²é¢†å–è¿‡" in message or "é‡å¤æäº¤" in message:
                    print(f'â„¹ï¸ èŠ‚ç‚¹ {node} å·²é¢†å–è¿‡/é‡å¤æäº¤ï¼Œå°è¯•ä¸‹ä¸€ä¸ªèŠ‚ç‚¹...')
                    if attempt_count == max_attempts or not available_nodes_for_url:
                         print(f'âŒ å·²è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•° ({max_attempts}) æˆ–æ— å¯ç”¨èŠ‚ç‚¹ï¼Œæäº¤å¤±è´¥')
                         # If max attempts reached, the last result (failure) will be recorded
                    continue # Continue the while loop to try the next node
                else:
                    # Other failure, record and break node loop
                    print(f'âŒ æäº¤å¤±è´¥ ({node}): {message}ï¼Œåœæ­¢å°è¯•è¯¥é“¾æ¥çš„å…¶ä»–èŠ‚ç‚¹')
                    break # Other failure, break the node attempt loop

            except Exception as e:
                print(f'âŒ å¤„ç†èŠ‚ç‚¹æ—¶å¼‚å¸¸ ({node}): {e}ï¼Œå°è¯•ä¸‹ä¸€ä¸ªèŠ‚ç‚¹')
                mihomo_proc.terminate()
                time.sleep(2) # Short delay after exception
                # Check if it was the last attempt
                if attempt_count == max_attempts or not available_nodes_for_url:
                     print(f'âŒ å·²è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•° ({max_attempts}) æˆ–æ— å¯ç”¨èŠ‚ç‚¹ï¼Œæäº¤å¤±è´¥')
                     # The last recorded message/success will be the exception message
                continue # Continue the while loop to try the next node

        # End of node attempt loop for the current URL
        result = {
            'url': url,
            'success': success,
            'message': message,
            'order_info': order_info,
            'submit_time': datetime.now().isoformat(),
            'node_used': node_used_for_success # Record node only on final success
        }
        results.append(result)
        if i < len(urls):
            delay = random.uniform(delay_min, delay_max)
            print(f'â³ ç­‰å¾… {delay:.1f} ç§’...')
            time.sleep(delay)
    # ç»Ÿè®¡
    success_count = sum(1 for r in results if r['success'])
    total_count = len(results)
    print(f"\nğŸ“Š æäº¤å®Œæˆ:")
    print(f"âœ… æˆåŠŸ: {success_count}/{total_count}")
    if total_count > 0:
        print(f"ğŸ“ˆ æˆåŠŸç‡: {(success_count/total_count*100):.1f}%")
    else:
        print("ğŸ“ˆ æˆåŠŸç‡: 0.0%ï¼ˆæ— æœ‰æ•ˆæäº¤ï¼‰")
    print('ğŸ›‘ mihomoå·²å…³é—­')

    # Save results to a JSON file
    try:
        with open('submission_results.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=4, ensure_ascii=False)
        print("âœ… è¯¦ç»†æäº¤ç»“æœå·²ä¿å­˜åˆ° submission_results.json")
    except Exception as e:
        print(f"âŒ ä¿å­˜ç»“æœåˆ°æ–‡ä»¶å¤±è´¥: {e}")

if __name__ == "__main__":
    main() 